Technial SEO: from HTML tags to URL

The technical aspect of SEO takes place in several steps and consists of checking the following points:

– the crawl of the site;
– indexing of web pages;
– the loading time of the pages;
– the internal linking;
– the structure of the site (not too deep).


#3.2 Compliant URL for SEO 

A URL is considered "non-compliant" when it does not meet the following criteria:

- Precense of an HTML page 
- presence of an HTTP 200 header
- presence of a canonical tag pointing towards itself 
- absence of meta noindex tags in strategic pages 


#3.9 Loaidng times by page type

To measure and analyze page loading times, we divide a site by page type:

– home page;
– category pages;
– sub-category pages;
– product or article pages.

TOOLS!!! -> such as Dareboost or Web Page Test

What is a good speed index? A good speed index (on mobile) is below 3,000. Here is what the score ranges mean:

– very good: less than 2,000;
– good: less than 3,000;
– fair: less than 5,000;
– bad: between 5,000 and 10,000;
– very bad: more than 10,000.




#3.10 robots.txt

Here is the syntax proposed by Google (also valid for other search engines) in its online documentation2:

– the robots.txt file must be an ASCII or UTF-8 text file. No other characters are allowed;
– a robots.txt file consists of one or more rules;
– each rule is composed of several directives (instructions) and only one directive per line is required.
A rule provides the following information:

– the robot (user-agent) to which the rule applies;
– the directories or files which this agent can access, if applicable;
– the directories or files which this agent cannot access, if applicable;
– rules are treated from top to bottom and the same user-agent can only fall under one rule, defined as the first most specific rule governing his behavior;
– the starting principle is that from the moment a page or directory is not blocked by a disallow rule, the user-agent can explore it;
– the rules are case sensitive


#Example 1: Only block Googlebot

User-agent: Googlebot

Disallow: /

#Example 2: Block Googlebot and Adsbot

User-agent: Googlebot

User-agent: AdsBot-Google

Disallow: /

#Example 3: Block all bots

User-agent: *

Disallow: /





#3.11 Sitemap.xml

The sitemap.xml helps to index a site faster. We recommend the following hierarchy to prioritize (in the <priority> tags of a sitemap) the indexing of pages in a site:

– home page: 1.0;
– category pages: 0.8;
– main product pages, bestsellers: 0.8;
– other product pages (ranges, secondary, etc.) : 0.7;
– other pages (company history, team, etc.): 0.5;
– legal pages (legal notices, terms and conditions, etc.): 0.2.


We also recommend having the tag <changefreq> for the frequency of page changes and to indicate the following elements for the types of pages concerned:

– home page: daily;
– news pages (blog, magazine, news, etc.): daily;
– category pages: weekly;
– product pages: daily;
– legal notices, terms and conditions, etc.: monthly.















